---
title: "HW 1"
author: "Shivalika Chavan"
date: "2026-02-15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet) 
library(caret) 
library(tidymodels) 
library(corrplot) 
library(ggplot2) 
library(plotmo) 
library(ggrepel)
library(pls)
```


```{r}
housing_train = read.csv("./data/housing_training.csv") |> 
  janitor::clean_names() |> 
  mutate(
    overall_qual = factor(overall_qual, 
                      levels = c("Very_Excellent",
                                 "Excellent", 
                                 "Very_Good", 
                                 "Good", 
                                 "Above_Average", 
                                 "Average", 
                                 "Below_Average", 
                                 "Fair"),
                      ordered = TRUE),
    kitchen_qual = factor(kitchen_qual, 
                      levels = c("Excellent", 
                                 "Good", 
                                 "Typical", 
                                 "Fair"),
                      ordered = TRUE),
    fireplace_qu = factor(fireplace_qu,
                      levels = c("Excellent", 
                                 "Good", 
                                 "Fair",
                                 "Poor", 
                                 "No_Fireplace"
                                 ),
                      ordered = TRUE),
    fireplace_qu = replace_na(fireplace_qu, "No_Fireplace"),
    exter_qual = factor(exter_qual, 
                      levels = c("Excellent", 
                                 "Good", 
                                 "Typical", 
                                 "Fair"),
                      ordered = TRUE)
  ) 

housing_test = read.csv("./data/housing_test.csv") |> 
  janitor::clean_names() |> 
  mutate(
    overall_qual = factor(overall_qual, 
                      levels = c("Very_Excellent",
                                 "Excellent", 
                                 "Very_Good", 
                                 "Good", 
                                 "Above_Average", 
                                 "Average", 
                                 "Below_Average", 
                                 "Fair"),
                      ordered = TRUE),
    kitchen_qual = factor(kitchen_qual, 
                      levels = c("Excellent", 
                                 "Good", 
                                 "Typical", 
                                 "Fair"),
                      ordered = TRUE),
    fireplace_qu = factor(fireplace_qu,
                      levels = c("Excellent", 
                                 "Good", 
                                 "Fair",
                                 "Poor", 
                                 "No_Fireplace"
                                 ),
                      ordered = TRUE),
    fireplace_qu = replace_na(fireplace_qu, "No_Fireplace"),
    exter_qual = factor(exter_qual, 
                      levels = c("Excellent", 
                                 "Good", 
                                 "Typical", 
                                 "Fair"),
                      ordered = TRUE)
  ) 

y = housing_train |> pull(sale_price) 
x = model.matrix(sale_price ~., housing_train)[,-1]

y_test = housing_test$sale_price
x_test = model.matrix(sale_price ~ ., housing_test)[,-1]
```

## a - LASSO Model

```{r}
set.seed(1234)

lambda = 10^(seq(-5, 5, 0.2)) # lambda grid of lambda values for penalty tuning

# k-fold cross-validation for Lasso (alpha = 1) over the lambda values
lasso_cv = cv.glmnet(x, y, 
                     alpha = 1, 
                     lambda = lambda)

lambda_min_glmnet = lasso_cv[["lambda.min"]] # minimum mean cross-validated error
lambda_1se_glmnet = lasso_cv[["lambda.1se"]] # largest value of lambda such that error is within 1 standard error of the cross-validated errors for lambda.min.
CVM_min = lasso_cv |> broom::tidy() |> filter(lambda == lambda_min_glmnet) |> pull(estimate)
CVM_1se = lasso_cv |> broom::tidy() |> filter(lambda == lambda_1se_glmnet) |> pull(estimate)
```

The $\lambda$ value with the smallest CVM (`r CVM_min`) is `r lambda_min_glmnet`. 
The $\lambda$ value with the 1SE CVM (`r CVM_1se`) is `r lambda_1se_glmnet`. 

```{r}
plot(lasso_cv)
plot_glmnet(lasso_cv$glmnet.fit)
```

Test Error with $\lambda$ = `r round(lambda_min_glmnet, 2)`:
```{r}
y_pred = predict(lasso_cv, newx = x_test, s = lambda_min_glmnet, type = "response")
mspe_lasso <- mean((y_test - y_pred)^2)
```

The test error is `r round(mspe_lasso, 2)`. 

```{r}
coef_1se = predict(lasso_cv, type = "coefficients", s = lambda_1se_glmnet)
# Count non-zero coefficients to determine the number of predictors (excluding intercept)
num_predictors_1se = sum(coef_1se != 0) - 1
```

When using $\lambda_{1SE}$, there are `r num_predictors_1se` predictors. 

## b - Elastic Net Model

```{r}
set.seed(1234)

alpha = seq(0, 1, length = 21) # alpha grid ranging from 0 (Ridge) to 1 (Lasso)
lambda = 10^(seq(-5, 5, 0.2)) # lambda grid of lambda values for penalty tuning

# Iterate through each alpha to perform cross-validation and store results in a tibble
enet_cv_results = tibble(alpha = alpha) |> 
  mutate(
    cv_fit = map(alpha, ~cv.glmnet(x, y, alpha = .x, lambda = lambda)), # runs glmnet for each alpha
    min_cvm = map_dbl(cv_fit, ~min(.x$cvm)) # finds min CVM for each model at each alpha
  )

# pull the alpha value with the lowest overall CVM
enet_alpha_min_cvm = enet_cv_results |> 
  filter(min_cvm == min(min_cvm)) |> 
  pull(alpha)

# pull the corresponding model with that optimal alpha
best_enet_cv_fit = enet_cv_results |> 
  filter(alpha == enet_alpha_min_cvm) |> 
  pull(cv_fit) |> 
  pluck(1)

# pull 1SE lambda for optimal alpha
lambda_1se_enet = best_enet_cv_fit$lambda.1se

# make predictions using optimal alpha and 1SE lambda at that alpha
y_pred = predict(best_enet_cv_fit, newx = x_test, s = "lambda.1se") 
mspe_elastic_net = mean((y_test - y_pred)^2)
```

The selected tuning parameters are $\alpha$ = `r round(enet_alpha_min_cvm, 2)` and $\lambda_{1SE}$ = `r round(lambda_1se_enet, 2)`. The model with these parameters has a test error of `r round(mspe_elastic_net, 2)`. 

Applying the 1SE rule is a bit more complicated with elastic net, because it has two parameters, $\alpha$ and $\lambda$. The 1SE rule, when used in the Lasso model, is easy to implement because a larger value for $\lambda$ means fewer non-zero coefficients and a more parsimonious model. In elastic net models, this will work when $\alpha$ is fixed at a single value.

## c - Partial Least Squares (PLS) Model

```{r}
set.seed(1234)

pls_mod <- plsr(sale_price ~ ., 
                data = housing_train, 
                scale = TRUE, # similar scaling importance as PCR
                validation = "CV")
summary(pls_mod)

# plot cross-validated MSEP for PLS
validationplot(pls_mod, val.type = "MSEP", legendpos = "topright")

# determine the optimal number of components
cv_mse <- RMSEP(pls_mod)
ncomp_cv <- which.min(cv_mse$val[1,,]) - 1
ncomp_cv

# calculate test MSE
predy2_pls <- predict(pls_mod, newdata = housing_test, 
                      ncomp = ncomp_cv)

mspe_pls = mean((y_test - predy2_pls)^2)
```

There are `r ncomp_cv |> unname()` components in the partial least squares model, with an MSPE of `r mspe_pls`.

## d - Comparing Models
```{r}

summary = tibble(
  model = c("LASSO", "Elastic Net", "PLS"),
  mspe = c(mspe_lasso, mspe_elastic_net, mspe_pls)
)

knitr::kable(summary)
```

Comparing the mean squared predicted error across the three models, the elastic net model is the best model for making predictions. This means that the model likely benefits from the balance of LASSO and Ridge ($\lambda$, $\alpha$) penalties, as compared to the LASSO model, which only has the $\lambda$ penalty. 

## e - Retraining LASSO model using caret instead of glmnet
```{r}
set.seed(1234)

ctrl1 = trainControl(method = "cv", number = 10) #cross validation, in this case 10-fold

lasso_caret = train(sale_price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = lambda),
                   trControl = ctrl1)
plot(lasso_caret, xTrans = log)

lambda_min_caret = lasso_caret$bestTune$lambda

tibble(
  package = c("caret (min)", "glmnet (min)", "glmnet (1SE)"),
  lambda = c(lambda_min_caret, lambda_min_glmnet, lambda_1se_glmnet)
) |> knitr::kable(digits = 2)
```

The "best" values for $\lambda$ differ slightly between `caret` and `cv.glmnet`. However, these two values are much closer to each other than they are to the 1SE value found using `cv.glmnet`. While they will result in slightly different models, their cross-validation errors will be within 1SE of each other. The reason for this discrepancy is the difference in internal functions; specifically, how each package assigns observations to k cross-validation folds, even when the random seed `set.seed(1234)` is kept consistent.